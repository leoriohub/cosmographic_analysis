{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f65e78-38b5-4500-8227-43cce02f77a2",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import rcParams\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import importlib\n",
    "\n",
    "\n",
    "rcParams[\"savefig.dpi\"] = 200\n",
    "rcParams[\"figure.dpi\"] = 200\n",
    "rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca54ea-bcd6-4559-b29c-02bc3480eb39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Important parameters\n",
    "\n",
    "#### **pts** is related to the number of points in the grid or resolution of the grid (don't change it)\n",
    "\n",
    "#### **q0f** & **h0f** are fiducial values for LCDM\n",
    "\n",
    "#### **repetitions** is the number of times hem_comp will be run (for testinf purposesm try for 5 repetitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a546ed06-99ef-4ac5-92c6-3bd41878952d",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#h0f = 0.678  # fiducial value of h0 for planck\n",
    "nside = 16\n",
    "pts = hp.nside2npix(nside)  # Resolution for Nside = 16\n",
    "q0f = -0.574\n",
    "h0f = 0.7304  # fiducial value of h0 for pantheon+\n",
    "zup = 0.1  # upper limit of the redshift range\n",
    "zdown = 0.00\n",
    "#zdown = 0.01\n",
    "repetitions = 500\n",
    "prefix_name = '[SH0ES_CALIB]'  # This is for file naming purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42032b3b-c08e-4f16-a26d-49b469fc0f42",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "loc_lcparam = 'datos/Pantheon+SH0ES.dat.txt'\n",
    "loc_lcparam_sys = 'datos/Pantheon+SH0ES_STAT+SYS.cov.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71eda28e-1e95-4e87-9ab2-517dcc3f4a63",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 741 SNe with  0.0 < z < 0.1\n",
      "\n",
      "Cov matrix shape: (741, 741)\n"
     ]
    }
   ],
   "source": [
    "lcparam = np.loadtxt(loc_lcparam, skiprows=1,\n",
    "                     usecols=(2, 8, 9, 10, 11, 26, 27, 12, 13))\n",
    "lcparam_sys = np.loadtxt(loc_lcparam_sys, skiprows=1)\n",
    "\n",
    "ind = np.where((lcparam[:, 0] < zup) & (lcparam[:, 0] > zdown))[0]\n",
    "\n",
    "zz = lcparam[ind, 0]  # zhd\n",
    "mz = lcparam[ind, 1]  # mb_corr\n",
    "sigmz = lcparam[ind, 2]  # m_b_corr_err_DIAG\n",
    "muz = lcparam[ind, 3]  # MU_SH0ES\n",
    "sigmuz = lcparam[ind, 4]  # MU_SH0ES_ERR_DIAG\n",
    "ra = lcparam[ind, 5]\n",
    "dec = lcparam[ind, 6]\n",
    "muceph = lcparam[ind, 7]\n",
    "hostyn = lcparam[ind, 8]\n",
    "hostyn_ind = np.where(hostyn == 1)[0]\n",
    "\n",
    "cov_z = lcparam_sys.reshape(1701, 1701)\n",
    "# Select only the cov matrix elements within the range.\n",
    "cov_z = cov_z[np.ix_(ind, ind)]\n",
    "inv_cov_z = np.linalg.inv(cov_z)\n",
    "\n",
    "print(\"We have\", len(zz), f\"SNe with  {zdown} < z < {zup}\")\n",
    "print(\"\\nCov matrix shape:\", cov_z.shape)\n",
    "\n",
    "cov_mat = pd.DataFrame(cov_z, columns=range(len(zz)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Cosmology model are defined in the file cosmology.py\n",
    "from cosmology import mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f080b-cc00-4ab0-87a9-32eb0be6f781",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Saving data into r1 array and positions (x,y,z) into v1 array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from healpix_vectors import DecRa2Cartesian\n",
    "# r1 is the data array\n",
    "r1 = np.column_stack([ra, dec, zz, mz, sigmz, muz, sigmuz, muceph, hostyn])\n",
    "\n",
    "# v1 contains all the SNe positions in the form of cartesian vectors.\n",
    "# For detailt check the file healpix_vectors.py\n",
    "\n",
    "v1 = DecRa2Cartesian(dec, ra)\n",
    "\n",
    "datos = (r1, v1, hostyn, cov_mat, h0f, q0f, pts, zup, zdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432aa3b2-e501-4721-baa0-11a9adbfb3ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Loop principal del mÃ©todo Hemispheric Comparison for fixed q0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faf8fc-abaa-4e89-bc33-870718727562",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Parallelization of the principal loop using Pool from multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126cd91-37c7-4153-b8ec-a54804967f3c",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# The function get_healpix_vectors is defined in healpix_vectors.py and it returns the symmetry axes for each pixel defined by HEALPIX\n",
    "from healpix_vectors import get_healpix_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from healpix_vectors import IndexToDecRa\n",
    "\n",
    "npix = hp.nside2npix(nside)\n",
    "pixel_indices = np.arange(npix)\n",
    "healpix_ra, healpix_dec = IndexToDecRa(nside, pixel_indices)\n",
    "healpix_theta, healpix_phi = hp.pix2ang(nside, pixel_indices)\n",
    "\n",
    "healpix_ra = np.array(healpix_ra)\n",
    "healpix_dec = np.array(healpix_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healpix_dirs = get_healpix_vectors(nside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import hem_comp_functions\n",
    "\n",
    "\n",
    "# We obtain the map by running hem_comp_functions.exec_map. This function runs the hemispheric comparison functions for each pixel to form a complete map.np.shape(IndexToDecRa(nside, pixel_indices))\n",
    "\n",
    "\n",
    "results_h0, results_q0 = hem_comp_functions.exec_map(healpix_dirs, datos, save=\"y\")\n",
    "\n",
    "h0u, h0d, h0u_err, h0d_err = results_h0\n",
    "q0u, q0d, q0u_err, q0d_err = results_q0\n",
    "\n",
    "\n",
    "# Concatenate the up and down results\n",
    "h0 = np.concatenate((h0u, h0d))\n",
    "h0_err = np.concatenate((h0u_err, h0d_err))\n",
    "q0 = np.concatenate((q0u, q0d))\n",
    "q0_err = np.concatenate((q0u_err, q0d_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(loadmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loadmap \n",
    "\n",
    "\n",
    "map_file = 'compilations/[NEW][MAP][SH0ES_CALIB](pts=3072_hf=0.678_qf=-0.574)(0.1>z>0.0).txt'\n",
    "h0u, h0u_err, h0d, h0d_err, q0u, q0u_err, q0d, q0d_err = loadmap.load_hubble_data(map_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0u = h0[:int(len(h0)/2)]\n",
    "h0d = h0[int(len(h0)/2):]\n",
    "\n",
    "q0u = q0[:int(len(q0)/2)]\n",
    "q0d = q0[int(len(q0)/2):]\n",
    "\n",
    "h0_erru = h0_err[:int(len(h0_err)/2)]\n",
    "h0_errd = h0_err[int(len(h0_err)/2):]\n",
    "\n",
    "q0_erru = q0_err[:int(len(q0_err)/2)]\n",
    "q0_errd = q0_err[int(len(q0_err)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543e0fb-e216-467b-bb43-0027afb9561e",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Store up and down hemisphere position vectors for plotting purposes.\n",
    "hdiru = np.array(healpix_dirs)\n",
    "hdird = -np.array(healpix_dirs)\n",
    "\n",
    "# save the directions into one array with the form [hdiru,hdird]\n",
    "hdirs = np.concatenate((hdiru, hdird))\n",
    "\n",
    "# convert the directions of axes into spherical coordinates\n",
    "theta = np.arccos(hdirs[:, 2])\n",
    "phi = np.radians(180) - np.arctan2(hdirs[:, 1], hdirs[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad1c51-da3f-4ccb-a14a-44127db32f53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Calculate error for $\\Delta q0$ and $\\Delta h0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Calculate the error for the maximum anisotropy\n",
    "\n",
    "h0_max_err = h0_err[np.argmax(h0)]\n",
    "h0_min_err = h0_err[np.argmin(h0)]\n",
    "\n",
    "q0_max_err = q0_err[np.argmax(q0)]\n",
    "q0_min_err = q0_err[np.argmin(q0)]\n",
    "\n",
    "\n",
    "delta_h0_data_err = np.sqrt(h0_max_err**2 + h0_min_err**2)\n",
    "delta_q0_data_err = np.sqrt(q0_max_err**2 + q0_min_err**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5e537-ac1c-4d49-ab70-0dbccc02fac7",
   "metadata": {
    "collapsed": false,
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Calculate max anisotroy for each parameter\n",
    "delta_h0_data = np.abs(np.array(h0u) - np.array(h0d))\n",
    "delta_q0_data = np.abs(np.array(q0u) - np.array(q0d))\n",
    "\n",
    "\n",
    "delta_h0_data_max = np.max(delta_h0_data)\n",
    "delta_q0_data_max = np.max(delta_q0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print max anisotropy with respective error\n",
    "\n",
    "print(\"delta_h0 = \", delta_h0_data_max, \"+/-\", delta_h0_data_err)\n",
    "print(\"delta_q0 = \", delta_q0_data_max, \"+/-\", delta_q0_data_err)\n",
    "\n",
    "#delta_h0_data_max = 0.017300021665283438\n",
    "#delta_q0_data_max = 1.369384233446544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b83602-02d2-44de-a5f7-c30d21f2e6a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Making q0 and h0 maps using HEALpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "rcParams[\"savefig.dpi\"] = 400\n",
    "rcParams[\"figure.dpi\"] = 400\n",
    "rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example on how to load a map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import max_anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfit_data = [h0u, h0d, q0u, q0d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_anis_dir = max_anisotropy.get_max_anisotropy(bestfit_data, healpix_dirs)\n",
    "print(\"The maximum anisotropy direction(dec,ra) for q0 is: \", max_anis_dir[0])\n",
    "\n",
    "max_h0_anis_theta = np.radians(90) - max_anis_dir[0][1]\n",
    "max_h0_anis_phi = np.radians(180) - max_anis_dir[0][0]\n",
    "\n",
    "max_q0_anis_theta = np.radians(90) - max_anis_dir[1][1]\n",
    "max_q0_anis_phi = np.radians(180) - max_anis_dir[1][0]\n",
    "\n",
    "max_q0_anis_vec = healpix_dirs[np.argmax(np.abs(delta_q0_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from generate_map import generate_map\n",
    "\n",
    "# Maps are generated by the function generate_map which is defined in generate_map.py\n",
    "# h0map, q0map = generate_map(nside, theta, phi, h0, q0, h0f=h0f, q0f=q0f)\n",
    "h0map_sn, q0map_sn = generate_map(nside, theta, phi, h0, q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "hp.mollview(h0map_sn, coord=\"cg\", title=rf'$h_0$ map (fixed $q_0$={q0f})', unit='', notext=True, norm='hist', cmap='jet',\n",
    "            min=min(h0map_sn), max=max(h0map_sn), fig=1, sub=(1, 3, 1))\n",
    "#hp.projplot(max_h0_anis_theta, max_h0_anis_phi, '^',\n",
    "            #color='white', markersize=10)  # Add a red dot\n",
    "\n",
    "hp.mollview(q0map_sn, coord=\"cg\", title=rf'$q_0$ map (fixed $h_0$={h0f})', unit='', notext=True, norm='hist', cmap='jet',\n",
    "            min=min(q0map_sn), max=max(q0map_sn), fig=1, sub=(1, 3, 2))\n",
    "hp.projplot(max_q0_anis_theta, max_q0_anis_phi,\n",
    "            '^', color='white', markersize=10)\n",
    "# also plot the opposite end\n",
    "hp.projplot((np.pi-max_q0_anis_theta) % (2*np.pi), -\n",
    "            max_q0_anis_phi, '^', color='white', markersize=10)\n",
    "\n",
    "#map_filename = f'figures/{prefix_name}[FULL](hf={h0f}_qf={q0f})({zup}>z>{zdown}).png'\n",
    "#plt.savefig(map_filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin-Watson statistic for all different directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dw_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(dw_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dw_up, dw_down = dw_statistic.hemispheric_dw(max_q0_anis_vec, datos)\n",
    "\n",
    "print(\"DW statistic for the maximum anisotropy direction\\n\")\n",
    "print(\"North:\", dw_up)\n",
    "print(\"South:\", dw_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durbin-Watson statistic for all HEALPIX directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dw_up, total_dw_down, data_dw = dw_statistic.total_dw(healpix_dirs, datos)\n",
    "print(\"Mean DW statistic for all directions\\n\")\n",
    "print(\"North:\", np.mean(total_dw_up))\n",
    "print(\"South:\", np.mean(total_dw_down))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330688ff-7354-4240-8831-b698a5caa6fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Synthetic data implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3db98-f7c4-4769-82e7-9ea28b7bc235",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Uniform supernovae distribution\n",
    "\n",
    "Below we have generated a random position vector for each supernova according to an uniform distribution. For more details refer to [Synthetic data on a sphere](<(1)synthetic_data.ipynb>).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d9e46-51ea-49e0-826b-03031e1903b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we want to create this len(ra) vectors for each repetition of the simulation and store them in a list\n",
    "repetitions = 500\n",
    "# We store the new supernova positions (x,y,z) in v1_iso\n",
    "v1_iso = np.zeros((repetitions, len(ra), 3))\n",
    "\n",
    "for i in range(repetitions):\n",
    "    vecti = np.random.randn(len(ra), 3)  # three random numbers vector (x,y,z)\n",
    "    # normalize the vector\n",
    "    vecti /= np.linalg.norm(vecti, axis=1)[:, np.newaxis]\n",
    "    thetai = np.arccos(vecti[:, 2])  # arccos(z)\n",
    "    phii = np.arctan2(vecti[:, 1], vecti[:, 0])  # arctan(y/x)\n",
    "    deci = thetai  # declination\n",
    "    rai = np.pi - phii  # right ascension\n",
    "    v1i = np.column_stack(\n",
    "        [np.sin(rai)*np.cos(deci), np.sin(rai)*np.sin(deci), np.cos(rai)])\n",
    "    v1_iso[i] = v1i\n",
    "v1_iso.shape\n",
    "\n",
    "# v1_iso is a list of lenght 500 repetitions which contains 741 vectors (x,y,z) for each supernovae position (isotropic synthetic distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are going to run the 500 repetitions for the synthetic uniformly distributed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8ee71-b15b-4d6b-b707-df921d2ed45c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h0u_iso = []\n",
    "h0d_iso = []\n",
    "q0u_iso = []\n",
    "q0d_iso = []\n",
    "\n",
    "\n",
    "i = 1\n",
    "for v1_it in v1_iso:\n",
    "    clear_output()\n",
    "    # Execute map with the given v1_iso element\n",
    "    datos_lcdm = [r1, v1_it, hostyn, cov_mat, h0f, q0f, pts, zup, zdown]\n",
    "    results_h0_iso_aux, results_q0_iso_aux = hem_comp_functions.exec_map(\n",
    "        healpix_dirs, tuple(datos_lcdm))\n",
    "\n",
    "    h0u_iso.append(np.array(results_h0_iso_aux[0]))\n",
    "    h0d_iso.append(np.array(results_h0_iso_aux[1]))\n",
    "    q0u_iso.append(np.array(results_q0_iso_aux[0]))\n",
    "    q0d_iso.append(np.array(results_q0_iso_aux[1]))\n",
    "    i += 1\n",
    "\n",
    "# Restore original v1 data as we are done.\n",
    "v1 = np.column_stack(\n",
    "    [np.sin(ra)*np.cos(dec), np.sin(ra)*np.sin(dec), np.cos(ra)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b285a7c-ec5a-47a4-8f05-f20521f2cdd6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert all arrays to numpy arrays\n",
    "h0u_iso = np.array(h0u_iso)\n",
    "h0d_iso = np.array(h0d_iso)\n",
    "q0u_iso = np.array(q0u_iso)\n",
    "q0d_iso = np.array(q0d_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4dbe1-75a6-4231-be01-43bb2e8d99c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fixed residuals in hem_comp_functions module\n",
    "h0m_iso = np.concatenate((h0u_iso, h0d_iso), axis=1)\n",
    "q0m_iso = np.concatenate((q0u_iso, q0d_iso), axis=1)\n",
    "\n",
    "# Find the maximum anisotropy for each repetition and store it\n",
    "delta_h0_iso_max = np.max(h0m_iso, axis=1) - np.min(h0m_iso, axis=1)\n",
    "delta_q0_iso_max = np.max(q0m_iso, axis=1) - np.min(q0m_iso, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data for all the repetitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_name = '[RESID]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the maximum anisotropy for each repetition\n",
    "filename = f'compilations\\{prefix_name}[ISO][Sh0ES_CALIB](hf={h0f}_qf={q0f})({zup}>z>{zdown}).txt' # type: ignore\n",
    "\n",
    "header = f'[SH0ES_CALIB]This is the data for ISO distributed Sne positions with a new set of data for each repetition.\\n{repetitions} repetitions, {pts} points, q0f= {q0f} h0f= {h0f}\\n\\ndelta_h0_iso_max            delta_q0_iso_max'\n",
    "save_iso_max_anisotropy = np.column_stack([delta_h0_iso_max, delta_q0_iso_max])\n",
    "\n",
    "np.savetxt(filename, save_iso_max_anisotropy, header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting the iterations\n",
    "\n",
    "We want to adjust a Gaussian to the data and include lines which represent the original data maximum anisotropy and some fixed percentiles in the same plot as the histogram.\n",
    "\n",
    "First, we fit a gaussian to the data. We do it using the function fit_gaussian in fit_gaussian.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fit_gaussian import fit_gaussian\n",
    "\n",
    "# We need to import the fit_gaussian function from fit_gaussian.py. This function is in charge or generating the gaussian for our data.\n",
    "\n",
    "\n",
    "# Fit a gaussian to the data using the fit_gaussian function. This function returns the x and y values of the gaussian.\n",
    "x_gauss_h0_iso, y_gauss_h0_iso = fit_gaussian(delta_h0_iso_max)\n",
    "x_gauss_q0_iso, y_gauss_q0_iso = fit_gaussian(delta_q0_iso_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_histograms\n",
    "import importlib\n",
    "importlib.reload(generate_histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_histograms\n",
    "data_h0_iso_hist = [delta_h0_iso_max, delta_h0_data_max, x_gauss_h0_iso, y_gauss_h0_iso]\n",
    "data_q0_iso_hist = [delta_q0_iso_max, delta_q0_data_max, x_gauss_q0_iso, y_gauss_q0_iso]\n",
    "\n",
    "file_name_iso = f'[RESID][ISO](hf={h0f}_qf={q0f})({repetitions})_rep_({zup}>z>{zdown}).png'\n",
    "\n",
    "generate_histograms.plot_histograms(data_h0_iso_hist, data_q0_iso_hist, filename=file_name_iso, titlemarker='ISO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978f761-40ae-48ad-bcd7-cb2218ee71bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Synthetic distance modulus (sampled from a normal distribution)\n",
    "\n",
    "### We are going to create a normal distribution with mean $\\mu_{fid}$ and $\\sigma$ will be the original error on the distance modulus.\n",
    "\n",
    "For more details on synthetic data, refer to [Synthethic data for distance modulus](synthetic_data.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = 500\n",
    "\n",
    "mu_fid = np.array([mu(z, h0f, q0f) for z in zz])  # 1D array 741\n",
    "# 3D array (repetitions, 741, 7) this is the grid of repetitions of r1 which will be modified to include the new mu_sample values.\n",
    "r1_lcdm = np.tile(r1, (repetitions, 1, 1))\n",
    "\n",
    "for i in range(repetitions):\n",
    "    mu_sample = np.random.normal(mu_fid, sigmuz)\n",
    "    r1_lcdm[i, :, 5] = mu_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3245db-b095-4fdf-9102-e32e76bdf497",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For each repetition we sample a new distance modulus for each supernovae from the normal distribution $N(\\mu_{fid}, sigmuz)$\n",
    "\n",
    "Where sigmuz is the original error on the distance modulus and $\\mu_{fid}$ is the fiducial value for the distance modulus.\n",
    "\n",
    "#### we call the function using he map method from pool object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb20d4b-9d8f-47a6-ac73-5fb57564a334",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hem_comp_functions\n",
    "\n",
    "h0um_lcdm = []\n",
    "h0dm_lcdm = []\n",
    "q0um_lcdm = []\n",
    "q0dm_lcdm = []\n",
    "\n",
    "i = 1\n",
    "for r1_it in r1_lcdm:\n",
    "    clear_output()\n",
    "    print('Repetition {}/{}'.format(i, repetitions))\n",
    "\n",
    "    datos_lcdm =[r1_it, v1 , hostyn, cov_mat, h0f, q0f, pts, zup, zdown]\n",
    "    results_h0_lcdm_aux, results_q0_lcdm_aux = hem_comp_functions.exec_map(healpix_dirs, tuple(datos_lcdm))\n",
    "\n",
    "    h0um_lcdm.append(np.array(results_h0_lcdm_aux[0]))\n",
    "    h0dm_lcdm.append(np.array(results_h0_lcdm_aux[1]))\n",
    "    q0um_lcdm.append(np.array(results_q0_lcdm_aux[0]))\n",
    "    q0dm_lcdm.append(np.array(results_q0_lcdm_aux[1]))\n",
    "    i += 1\n",
    "\n",
    "# Restore original r1 data\n",
    "\n",
    "r1 = np.column_stack([ra, dec, zz, mz, sigmz, muz, sigmuz, muceph, hostyn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0um_lcdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b6ca0-86e5-42a0-b88b-545d6c348389",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h0um_lcdm = np.array(h0um_lcdm)\n",
    "h0dm_lcdm = np.array(h0dm_lcdm)\n",
    "q0um_lcdm = np.array(q0um_lcdm)\n",
    "q0dm_lcdm = np.array(q0dm_lcdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea6782-ea76-4134-a664-5f24342c596f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h0m_lcdm = np.concatenate((h0um_lcdm, h0dm_lcdm), axis=1)\n",
    "q0m_lcdm = np.concatenate((q0um_lcdm, q0dm_lcdm), axis=1)\n",
    "\n",
    "delta_h0_lcdm_max = np.max(h0m_lcdm, axis=1) - np.min(h0m_lcdm, axis=1)\n",
    "delta_q0_lcdm_max = np.max(q0m_lcdm, axis=1) - np.min(q0m_lcdm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fit_gaussian import fit_gaussian\n",
    "\n",
    "# We need to import the fit_gaussian function from fit_gaussian.py. This function is in charge or generating the gaussian for our data.\n",
    "\n",
    "\n",
    "# Fit a gaussian to the data using the fit_gaussian function. This function returns the x and y values of the gaussian.\n",
    "x_gauss_h0_lcdm, y_gauss_h0_lcdm = fit_gaussian(delta_h0_lcdm_max)\n",
    "x_gauss_q0_lcdm, y_gauss_q0_lcdm = fit_gaussian(delta_q0_lcdm_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_histograms import plot_histograms\n",
    "\n",
    "data_h0_lcdm_hist = [delta_h0_lcdm_max,delta_h0_data_max, x_gauss_h0_lcdm, y_gauss_h0_lcdm]\n",
    "data_q0_lcdm_hist = [delta_q0_lcdm_max,delta_q0_data_max, x_gauss_q0_lcdm, y_gauss_q0_lcdm]\n",
    "\n",
    "\n",
    "file_name_lcdm = f'{prefix_name}[LCDM](hf={h0f}_qf={q0f})({repetitions})_rep_({zup}>z>{zdown}).png'\n",
    "plot_histograms(data_h0_lcdm_hist, data_q0_lcdm_hist, titlemarker='LCDM', filename=file_name_lcdm)\n",
    "#plot_histograms(data_h0_lcdm_hist, data_q0_lcdm_hist, titlemarker='LCDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24ce6e-79b1-402b-841c-b7a57098b0e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot both histrograms for LCDM and ISO in the same plot using the plot_both_histograms function\n",
    "from generate_histograms import plot_both_histograms\n",
    "\n",
    "data_h0 = np.array([delta_h0_iso_max, delta_h0_lcdm_max, delta_h0_data_max])\n",
    "data_q0 = np.array([delta_q0_iso_max, delta_q0_lcdm_max, delta_q0_data_max])\n",
    "\n",
    "file_name_both = f'{prefix_name}[BOTH](hf={h0f}_qf={q0f})({repetitions})_rep_({zup}>z>{zdown}).png'\n",
    "\n",
    "plot_both_histograms(data_h0, data_q0, filename=file_name_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401ab6e-d6aa-4fe5-92a1-ac1981ab1433",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Saving the results in text files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf8d0c-ebe2-4850-bf1a-a2a19552dda9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save ISO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d618531-71cb-48b8-b698-deeeb91e9dc6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header_iso = f'This is the data for ISO distribution with new set of data for each repetition\\n{repetitions} repetitions, {pts} points q0f= {q0f} h0f= {h0f}\\n\\ndelta_h0_iso_max  delta_q0_iso_max'\n",
    "\n",
    "filename_iso = f'compilations/{prefix_name}[IFADESKTOP][ISO]({h0f}=h0f_{q0f}=q0f)({repetitions}_rep)({pts}_pts)({zup}>z>{zdown}).txt'\n",
    "\n",
    "save_data_iso = np.column_stack([delta_h0_iso_max, delta_q0_iso_max])\n",
    "\n",
    "np.savetxt(filename_iso, save_data_iso, header=header_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989f038-8712-42f7-9f57-ae0c8cb1ecc5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save LCDM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce473b8c-2779-4330-9935-d2349c3aa0a7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header_lcdm = f'[SH0ES_CALIB] This is the data for LCDM distribution with a new set of data for each repetition\\n{repetitions} repetitions, {pts} points, q0f= {q0f} h0f= {h0f}\\n\\ndelta_h0_lcdm_max            delta_q0_lcdm_max'\n",
    "\n",
    "filename_lcdm = f'compilations/{prefix_name}[LCDM]({h0f}=h0f_{q0f}=q0f)({repetitions}_rep)({pts}_pts)({zup}>z>{zdown}).txt'\n",
    "\n",
    "save_data_lcdm = np.column_stack([delta_h0_lcdm_max, delta_q0_lcdm_max])\n",
    "\n",
    "np.savetxt(filename_lcdm, save_data_lcdm, header=header_lcdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947418a0-38fa-425b-919b-f966fcc49e7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load the results from text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb8807-951c-4c4e-8e73-230a8f271189",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path_iso = 'compilations/[SH0ES_CALIB][ISO](0.7304=h0f_-0.574=q0f)(500_rep)(3072_pts)(0.1>z>0.01).txt'\n",
    "\n",
    "# file_path_iso = 'compilations/[SH0ES_CALIB][HEALPIX][ISO](0.7304=h0f_-0.574=q0f)(500_rep)(1536_pts)(0.1>z>0.0).txt'\n",
    "\n",
    "\n",
    "data_iso = np.loadtxt(file_path_iso, usecols=(0, 1), skiprows=4)\n",
    "\n",
    "delta_h0_iso_max = data_iso[:, 0]\n",
    "delta_q0_iso_max = data_iso[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db40bf0-2499-4364-a50c-4db1846aca61",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path_lcdm = 'compilations/[SH0ES_CALIB][LCDM](0.7304=h0f_-0.574=q0f)(500_rep)(3072_pts)(0.1>z>0.01).txt'\n",
    "# file_path_lcdm = 'compilations/[SH0ES_CALIB][HEALPIX][LCDM](0.7304=h0f_-0.574=q0f)(500_rep)(3072_pts)(0.1>z>0.0).txt'\n",
    "\n",
    "data_lcdm = np.loadtxt(file_path_lcdm, usecols=(0, 1), skiprows=4)\n",
    "\n",
    "delta_h0_lcdm_max = data_lcdm[:, 0]\n",
    "delta_q0_lcdm_max = data_lcdm[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.dpi'] = 400\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import the fit_gaussian function from fit_gaussian.py. This function is in charge or generating the gaussian for our data.\n",
    "from fit_gaussian import fit_gaussian\n",
    "\n",
    "\n",
    "# Fit a gaussian to the data using the fit_gaussian function. This function returns the x and y values of the gaussian to be plotted.\n",
    "x_gauss_h0_iso, y_gauss_h0_iso = fit_gaussian(delta_h0_iso_max)\n",
    "x_gauss_q0_iso, y_gauss_q0_iso = fit_gaussian(delta_q0_iso_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_histograms import plot_histograms\n",
    "\n",
    "data_h0_iso_hist = [delta_h0_iso_max, delta_h0_data_max, x_gauss_h0_iso, y_gauss_h0_iso]\n",
    "data_q0_iso_hist = [delta_q0_iso_max, delta_q0_data_max, x_gauss_q0_iso, y_gauss_q0_iso]\n",
    "\n",
    "file_name_iso = f'{prefix_name}[ISO](hf={h0f}_qf={q0f})({repetitions})_rep_({zup}>z>{zdown}).png'\n",
    "\n",
    "\n",
    "plot_histograms(data_h0_iso_hist, data_q0_iso_hist, filename=file_name_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gauss_h0_lcdm, y_gauss_h0_lcdm = fit_gaussian(delta_h0_lcdm_max)\n",
    "x_gauss_q0_lcdm, y_gauss_q0_lcdm = fit_gaussian(delta_q0_lcdm_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_q0_lcdm_max.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_histograms import plot_histograms\n",
    "\n",
    "data_h0_lcdm_hist = [delta_h0_lcdm_max,delta_h0_data_max, x_gauss_h0_lcdm, y_gauss_h0_lcdm]\n",
    "data_q0_lcdm_hist = [delta_q0_lcdm_max,delta_q0_data_max, x_gauss_q0_lcdm, y_gauss_q0_lcdm]\n",
    "\n",
    "\n",
    "file_name_lcdm = f'{prefix_name}[LCDM](hf={h0f}_qf={q0f})({repetitions})_rep_({zup}>z>{zdown}).png'\n",
    "\n",
    "plot_histograms(data_h0_lcdm_hist, data_q0_lcdm_hist, titlemarker='LCDM', filename=file_name_lcdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_both = f'histograms/{prefix_name}[BOTH]({h0f}=h0f_{q0f}=q0f)({repetitions}_rep)({pts}_pts)({zup}>z>{zdown}).txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_histograms\n",
    "\n",
    "importlib.reload(generate_histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both histrograms for LCDM and ISO in the same plot using the plot_both_histograms function\n",
    "from generate_histograms import plot_both_histograms\n",
    "\n",
    "data_h0 = [delta_h0_iso_max, delta_h0_lcdm_max, delta_h0_data_max]\n",
    "data_q0 = [delta_q0_iso_max, delta_q0_lcdm_max, delta_q0_data_max]\n",
    "\n",
    "file_name_both = f'{prefix_name}[BOTH](hf={h0f}_qf={q0f})({repetitions})_rep_({zup}>z>{zdown}).png'\n",
    "\n",
    "plot_both_histograms(data_h0, data_q0, filename=file_name_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b753b13-b8a3-4646-b5ed-5c6802bf5081",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Some statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433b9aa-6c9b-48e6-89ed-201c7d758654",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import get_statistics\n",
    "\n",
    "# This function takes all maximum anisotropy for each LCDM and ISO and returns the pvalues\n",
    "\n",
    "maximum_anisotropy_data = np.array([delta_h0_data_max, delta_q0_data_max])\n",
    "maximum_anisotropy_mc = np.array([delta_h0_lcdm_max, delta_q0_lcdm_max, delta_h0_iso_max, delta_q0_iso_max])\n",
    "\n",
    "p_values = get_statistics.mc_statistics(maximum_anisotropy_data, maximum_anisotropy_mc)\n",
    "\n",
    "p_h0_iso_max = p_values[0]\n",
    "p_q0_iso_max = p_values[1]\n",
    "p_h0_lcdm_max = p_values[2]\n",
    "p_q0_lcdm_max = p_values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0981828-050b-45fc-a26c-7d429f2e5874",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.dpi'] = 300\n",
    "z_ranges = [f\"{zup} > z > {zdown}\"]\n",
    "\n",
    "delta_h = [f\"{delta_h0_data_max:.4f}\"]\n",
    "iso_pvalues = [f\"{p_h0_iso_max:.4f}\"]\n",
    "lcdm_pvalues = [f\"{p_h0_lcdm_max:.4f}\"]\n",
    "\n",
    "delta_q0 = [f\"{delta_q0_data_max:.4f}\"]\n",
    "iso_q0_pvalues = [f\"{p_q0_iso_max:.4f}\"]\n",
    "lcdm_q0_pvalues = [f\"{p_q0_lcdm_max:.4f}\"]\n",
    "\n",
    "# Crear el DataFrame para el mapa de Hubble\n",
    "df_hubble = pd.DataFrame({\"z range\": z_ranges, \"delta h\": delta_h,\n",
    "                         \"ISO p-value\": iso_pvalues, \"LCDM p-value\": lcdm_pvalues})\n",
    "\n",
    "# Crear el DataFrame para los valores de q0\n",
    "df_q0 = pd.DataFrame({\"z range\": z_ranges, \"delta q0\": delta_q0,\n",
    "                     \"ISO p-value\": iso_q0_pvalues, \"LCDM p-value\": lcdm_q0_pvalues})\n",
    "\n",
    "# Crear la figura y el eje para la tabla del mapa de Hubble\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=df_hubble.values, colLabels=df_hubble.columns, loc='center')\n",
    "\n",
    "# Guardar la tabla del mapa de Hubble en un archivo PNG\n",
    "plt.savefig(\n",
    "    f'tables/{prefix_name}[h0_table](hf={h0f}_qf={q0f})_({zup}>z>{zdown})({pts}_pts)_({len(delta_h0_iso_max)})_rep_.png')\n",
    "\n",
    "# Crear la figura y el eje para la tabla de q0\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "ax.table(cellText=df_q0.values, colLabels=df_q0.columns, loc='center')\n",
    "\n",
    "# Guardar la tabla de q0 en un archivo PNG\n",
    "plt.savefig(\n",
    "    f'tables/{prefix_name}[q0_table](hf={h0f}_qf={q0f})_({zup}>z>{zdown})({pts}_pts)_({len(delta_h0_iso_max)})_rep_.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "name": "[SH0ES_CALIB][HEALPIX]A_model_independent_test(1.0.0).ipynb",
  "vscode": {
   "interpreter": {
    "hash": "df2357fdaf535833a8648acd2ab9d3b392e8fdf3dc265ac40e4495478976a592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
